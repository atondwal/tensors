\documentclass[11pt,notitlepage]{article}
\usepackage{amsfonts, amsmath, amssymb, amsthm,fullpage,mdwlist,graphicx,cancel}
\setlength{\textheight}{9.25in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{0.0in}
\setlength{\headheight}{0.0in}
\setlength{\headsep}{0.0in}
\setlength{\leftmargin}{0.0in}
\setlength{\oddsidemargin}{0.0in}
\setlength{\parindent}{0pc}
\everymath{\displaystyle}
\newtheorem{thm}{Theorem}[section]
\newtheorem{exc}{Exercise}[section]
\title{Tensors and Co-/Contra- Variance}
\author{Anish Tondwalkar}
\date{\today}
\begin{document}
\maketitle
In physics, all of our measurements are going to correspond to scalars (specifically, real numbers). 
Furthermore, because our physical measurements can't depend on our mathematical conventions and choice of coordinate system,
these scalars much be the same (invariant) under rotations of the coordinate system.
\section{Contravariance}
Vectors, however, aren't tied to the events we're measuring by their coordinates in our coordinate system, but by their geometric meaning.
We can see, by trivial example that a vector transforms `away' from our rotation. 
Thus we will call things that transform this way `contravariant'
\footnote{Indices for contravariant vectors go at the top. This is different from the lower index notation you've probably been using all your life, because in Euclidean spaces, the covariant and contravariant form of the vector are the same, so we can just write everything covariantly, because it's less ambiguous notation}. 
\begin{exc}
Find a coordinate system in which
$\hat\i + \hat\j + \hat k$
 can be written as $\sqrt3 \hat\j$
\end{exc}
In order to go from one coordinate system to another, you should know that we use the cosines of the angle between them:
$$ x'^i = \sum_j x^j \cos(x^j,x'^i) $$
Written with differentials, we have
$$ dx'^i = \sum_j \frac{\partial x'^i}{\partial x^j} dx^j $$
Giving us our transformation law for contravariant tensors:
$$ A'^i = \sum_j \frac{\partial x'^i}{\partial x^j} A^j $$
\section{Covariance}
But that's not all! We know things that transform the other way: gradients:
$$ \nabla \psi = \sum_j \frac{\partial \psi}{\partial x^j} \hat x^j $$
 They're our prototype for covariant forms. 
From the chain rule, we have: 
$$\frac{\partial \psi}{\partial x'^i} = \sum_j \frac{\partial \psi}{\partial x^j} \frac{\partial x^j}{\partial x'^i} $$
So out covariant transformation law is:
$$ A_i = \sum_j \frac{\partial x^j}{\partial x'^i} A_j$$

\section{n-forms and Tensors}
In k-space, we already know that scalars have $k^0=1$ components that are invariant, and that vectors have $k^1=k$ components that use one of these transformations relations. We can generalize this to something with $k^n$ components that transform in a definite way. n-forms have n covariant indices, and tensors of rank n, have n indices of either variance. Therefore, we can say that scalars are tensors of rank 0 and that vectors are contravariant tensors of rank 1. For example a mixed tensor of rank 2 transforms like this \footnote{Here, we suppress the sigmas. See Einstein Notation}:
$$A'^i_j =  \frac{\partial x'^i}{\partial x^k}\frac{\partial x^l}{\partial x'^j} A^k_l $$
\section{Contraction, Direct Products and the Quotient Rule}
When we take the direct product of two tensors, we add up the covariant indices and the contravariant indices, and get a tensor of a higher rank. We can contract a covariant index with a contravariant index, to get a tensor of a lower rank by making the indices the same. This should be obvious from the transformation laws, because when we multiply the tensors, we multiply the components and the coefficients that arrive from the transformation, so the number of these adds, and furthermore, the direction cosine from one vector in the basis with itself is 1. 
This implies that when we have a relationship involving tensors, the number of covariant and contravariant indices on each side of the equal sign must be the same. This is the quotient rule of tensors.
\end{document}

